<?xml version="1.0" encoding="UTF-8"?>
<SnapshotIndexes Version="1.0" BinderUUID="1FA218EA-B4C1-4D21-B2D0-A35B75920BBE">
    <Snapshot Date="2018-08-30 08:47:42 +0200">
        <Title>Still includes semiotic differences between human and software semantics, and the grounding problem</Title>
        <Text>This shows how the cognitive quality of the conceptualisation could be substituted with a formulation in set theory. The resulting conceptual model essentially remains a representation, albeit a mathematical one. One can argue that such substitution does not resolve the grounding problem, and appropriately so. Still, mathematics provides for a very exact way to express oneself, reducing the ambiguity that comes implicitly with any other language. Furthermore, logical constructs used at the syntactical level can be interpreted into set theoretic operations, facilitating the evaluation of complicated expressions. And thanks to mathematics we can also indicate the exact issues that exist with conceptual modelling, depicted in \cref{fig:4-model-construct-issues}.

![Four different types of construction issues that come with formal semantics][def:constructissues] 

This begs the question what we mean with model, and what criteria we should adopt to represent a conceptual model. This is especially relevant since in contemporary architectural paradigms models are being used as first class citizens to the architectures, MDA and ISP RM/ODP alike. 




An appropriate definition for ontology is given by \cite{Guarino:1998wq} as a “logical theory accounting for the intended meaning of a formal vocabulary”.



The triadic model is more suitable to explain the differences between human semantics and semantics in computers, by identifying the semiotic differences between the two as follows. Since humans are capable of making observations from reality, and abstract these into conceptualisations, there is a direct connection between the entity and the conceptualisation. Computers lack that capability, as depicted in \cref{fig:semiotic-differences}. Here, we show the semiotic differences between semantics as they appear for human actors, part (a) of the Figure, and that of software actors in part (b). The comparison is made from the perspective of communication, e.g., how is reality signified into utterances made by the actor, and vice versa, how are utterances signified into what they stand for in reality. We can assume the entity to remain identical over both actors, and the token to remain equivalent to the extent that in terms of computers these are referred to as *data*. The third node, representing the conceptualisation for human agents, for software agents we claim to denote that as the application. Although in its bare form an application is nothing more than tokens that follow a specific language grammar, this bare form is only a representation of its quintessence, i.e., a run-time notion on how to act on the receipt of data.

However, because computers are unable to conceptualise or concretize, the connection between the software’s conceptualisation and the entity does not exist. This “missing link” in artificial intelligence is called *the grounding problem*, named after the inability to ground a conceptualisation in what it refers to in reality. In literature, two exceptions to this rule exist, which we discuss in the box text below. Our stance towards these exceptions is that they are interesting, however currently irrelevant towards the resolution of semantic interoperability due to their many practical shortcomings in implementing an actual connection between the entity and the conceptualisation. 


![Semiotic differences in semantics for humans and computers][def:semdiffs]





This is known as the *problem of reference*, a manifestation of the *grounding problem*. 

In information systems, addressing the distinction between terms and reality is extremely limited [@Steels:2008tr], or not present at all [@Cregan2007]. 

Artificial intelligence (AI) tries to tackle the grounding problem by building some form of understanding, also known as "strong AI". However, strong AI is expected to emerge on the long term only, if ever [@XiuquanLi2017]. Its counterpart "weak AI", characterised by logic and reasoning, relies on language only and can therefore never make the step to reality on its own [@Scheider:2012tj]. 


Hierin duidelijk maken wat de verschillen zijn tussen modellen en ontologie. Semiotiek (eigenlijk de semiotische driehoek) gebruiken wij als methode om te verklaren wat semantiek is bij mensen en bij computers. En zonder semantiek in de architectuur, geen sIOP. 

CONCLUSIE: Architectures will not be able to facilitate semantics and, hence, consolidate sIOP without including semiotics.
Assumption 1: root cause for sIOP issues is the grounding problem: GP leads to absence of semantics, absence of semantics leads to absence of sIOP.
Fact: Strong AI could solve GP, but doesn’t exist
Fact: Weak AI is based on language only, and can never solve GP on its own 
Observation: Humans can solve GP, semiotics explain why
Fact: Semiotics studies relation between language (terms) and meaning


Thus, weak AI is our only option for the time being in order to achieve semantics and sIOP. 


We therefore cannot neglect the existence of the grounding problem and its semiotic origins. Nevertheless, we do. For instance, when we are asked to explain how we address the grounding problem in the design of our software agent, we can’t; when we are asked to point at the semantics parts in the code of our software agent, we can't. The same question however about, e.g., its scalability, will render a lecture with adequate references to the underlying architecture. We thus remain at a loss of how to engineer semantics into software agents. However, without a clear understanding on semantics and its contribution to the software agent, we are lacking the bridgehead within the software agent that is fundamental to the semantic interoperability bridge. 






In fact, this is a question of philosophy while ICT is “only” faced with its consequence: computers can deal with language only and have no clue about reality. 






It therefore remains impossible to ground the applied terms in reality, denoted as the *grounding problem*. Its resolution is a major subject in strong AI and in (geographic) information science in general \cite{Scheider:2012tj}. Although \cite{Steels:2008tr} provides for an alternative (weak AI) solution, that only shows the need to refer to  general stance is that the grounding problem remains a big challenge .




&lt;!-- page additions --&gt;
[def:semdiffs]: src\images\SemioticDifferences.png {#fig:semiotic-diffs }
</Text>
    </Snapshot>
</SnapshotIndexes>