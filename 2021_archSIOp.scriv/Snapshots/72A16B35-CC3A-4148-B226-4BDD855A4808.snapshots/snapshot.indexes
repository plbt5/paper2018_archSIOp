<?xml version="1.0" encoding="UTF-8"?>
<SnapshotIndexes Version="1.0" BinderUUID="72A16B35-CC3A-4148-B226-4BDD855A4808">
    <Snapshot Date="2018-08-17 17:08:24 +0200">
        <Title>before re-integrating introductory section "what is software semantics"</Title>
        <Text>

1. Explain shortcomings of 42010:2011 in terms of semiotic triangle:
    1. All models are representations of engineers’ conceptualisations
    1. In MDA, “models represent reality” makes the semiotic triangle conflate in a [|model] &lt;—[|representation]—&gt; [|reality] dimension, 
    1. This cuts-off the conceptualisation vertex and with that our “knowledge about our given remark or doctrine *says* there is”. We have removed the “ontological level” [@Guarino1994b], and with that, the fact that “terminological competence can be gained by formally expressing the ontological commitment of a knowledge base” (ibid.)
    1. (Meta-)model instantiation, and hence level transition, therefore remains at the Term/Model vertex
    1. The CIM models both semantics (Domain Model) and pragmatics (Business Model)
    1. Models, including the CIM, are validated in other models, while ontologies are interpreted in their conceptualisation (sets and set theory)
    1. Models are ultimately expressed as either Data or Code, both located at the Term/Model vertex.
1. Clarify that the reciprocity between code and data manifests itself as software semantics
    1. The relationship between Data and Code is very closely coupled in order to maintain consistency between each other. Inconsistency results in software malfunction or crashes. Maintaining/controlling that consistency is one of the main goals of MDA/MDE.
    1. Inconsistency between Code and Data has either pragmatic grounds (i.e., code assumes different reality than data resulting in incorrect operations on the data) or semantic grounds (i.e., data assumes different reality than the code resulting in incorrect data being correctly operated on). 






An appropriate definition for ontology is given by \cite{Guarino:1998wq} as a “logical theory accounting for the intended meaning of a formal vocabulary”.



The triadic model is more suitable to explain the differences between human semantics and semantics in computers, by identifying the semiotic differences between the two as follows. Since humans are capable of making observations from reality, and abstract these into conceptualisations, there is a direct connection between the entity and the conceptualisation. Computers lack that capability, as depicted in \cref{fig:semiotic-differences}. Here, we show the semiotic differences between semantics as they appear for human actors, part (a) of the Figure, and that of software actors in part (b). The comparison is made from the perspective of communication, e.g., how is reality signified into utterances made by the actor, and vice versa, how are utterances signified into what they stand for in reality. We can assume the entity to remain identical over both actors, and the token to remain equivalent to the extent that in terms of computers these are referred to as *data*. The third node, representing the conceptualisation for human agents, for software agents we claim to denote that as the application. Although in its bare form an application is nothing more than tokens that follow a specific language grammar, this bare form is only a representation of its quintessence, i.e., a run-time notion on how to act on the receipt of data.

However, because computers are unable to conceptualise or concretize, the connection between the software’s conceptualisation and the entity does not exist. This “missing link” in artificial intelligence is called *the grounding problem*, named after the inability to ground a conceptualisation in what it refers to in reality. In literature, two exceptions to this rule exist, which we discuss in the box text below. Our stance towards these exceptions is that they are interesting, however currently irrelevant towards the resolution of semantic interoperability due to their many practical shortcomings in implementing an actual connection between the entity and the conceptualisation. 


![Semiotic differences in semantics for humans and computers][def:semdiffs]





This is known as the *problem of reference*, a manifestation of the *grounding problem*. 

In information systems, addressing the distinction between terms and reality is extremely limited [@Steels:2008tr], or not present at all [@Cregan2007]. 

Artificial intelligence (AI) tries to tackle the grounding problem by building some form of understanding, also known as "strong AI". However, strong AI is expected to emerge on the long term only, if ever [@XiuquanLi2017]. Its counterpart "weak AI", characterised by logic and reasoning, relies on language only and can therefore never make the step to reality on its own [@Scheider:2012tj]. 


Hierin duidelijk maken wat de verschillen zijn tussen modellen en ontologie. Semiotiek (eigenlijk de semiotische driehoek) gebruiken wij als methode om te verklaren wat semantiek is bij mensen en bij computers. En zonder semantiek in de architectuur, geen SIOp. 

CONCLUSIE: Architectures will not be able to facilitate semantics and, hence, consolidate SIOp without including semiotics.
Assumption 1: root cause for SIOp issues is the grounding problem: GP leads to absence of semantics, absence of semantics leads to absence of SIOp.
Fact: Strong AI could solve GP, but doesn’t exist
Fact: Weak AI is based on language only, and can never solve GP on its own 
Observation: Humans can solve GP, semiotics explain why
Fact: Semiotics studies relation between language (terms) and meaning


Thus, weak AI is our only option for the time being in order to achieve semantics and SIOp. 


We therefore cannot neglect the existence of the grounding problem and its semiotic origins. Nevertheless, we do. For instance, when we are asked to explain how we address the grounding problem in the design of our software agent, we can’t; when we are asked to point at the semantics parts in the code of our software agent, we can't. The same question however about, e.g., its scalability, will render a lecture with adequate references to the underlying architecture. We thus remain at a loss of how to engineer semantics into software agents. However, without a clear understanding on semantics and its contribution to the software agent, we are lacking the bridgehead within the software agent that is fundamental to the semantic interoperability bridge. 






In fact, this is a question of philosophy while ICT is “only” faced with its consequence: computers can deal with language only and have no clue about reality. 






It therefore remains impossible to ground the applied terms in reality, denoted as the *grounding problem*. Its resolution is a major subject in strong AI and in (geographic) information science in general \cite{Scheider:2012tj}. Although \cite{Steels:2008tr} provides for an alternative (weak AI) solution, that only shows the need to refer to  general stance is that the grounding problem remains a big challenge .

despite the notoriously difficult philosophical questions involved, semantic interoperability can be seen as an engineering problem, namely
that of effectively constraining interpretations towards the ones that are considered
allowable.


&lt;!-- page additions --&gt;
[def:semdiffs]: src\images\SemioticDifferences.png {#fig:semiotic-diffs }
</Text>
    </Snapshot>
    <Snapshot Date="2019-10-16 12:58:43 +0200">
        <Title>Darlings that are killed</Title>
        <Text>Despite the precise meaning of the term ‘semantics’ in semantic interoperability, it is clear that it encompasses a communication between at least two actors. This brings a natural responsibility for both actors in the communication, described by [@Grice:1991BT] as the particular purpose of communication, viz. to serve:

1. Quantity: Make your contributions as informative as is required (for the current purpose of the exchange), and not more than is required;
2. Quality: Do not say what you believe to be false, or for which you lack evidence;
3. Relation: Be relevant (to the immediate needs);
4. Manner: Avoid obscurity of expression, ambiguity, and be brief and orderly.

This leads to the definition of a design principle to its effect, applying the normative notation from [@Greefhorst2011]:

\begin{mmdp}[Principle of responsibility for meaning]\label{dp:responsibility-for-meaning}

When it is reasonable to expect that the software agent will be engaged in collaboration or otherwise will interoperate with (an)other software agent(s), it is the responsibility of the software architect to serve the quantity, relation and manner of the interoperability by specifying the semantics of the data it produces in advance.   

\textbf{Type of information:} business  \\
\textbf{Quality attributes:} semantics, semantic interoperability   \\
\textbf{Rationale:}
\begin{enumerate}
  \item Data represent the state of affairs of some part of the world, viewed from a particular perspective of use. Such view is just one particular perspective out of many equally legitimate ones;
  \item Semantic heterogeneity, a direct consequence of the equally legitimate perspectives on reality, should not be considered a bug to resolve, but a feature to preserve and nurture in order to maximise semantic accuracy and relevancy;
  \item Accepting semantic heterogeneity implies the probable uniqueness of the agents view on reality;
  \item Computers are not capable of genuine understanding, hence cannot establish semantics from data and thus require the human-in-the-loop for that;
  \item The responsibility for formulating the semantics that are expressed by the data can only lay by the software architect that has taken the particular perspective on reality when carving out the entities of interest to the software application;
  \item On specifying semantics, Grice’s maxims on communication, and particularly on serving the quantity, relation and manner of communication, represent the natural constraints to respect;
  \item Without adherence to this principle, the meaning of the data expressed by the software agent can be considered flawed, inaccurate, incomplete or otherwise insufficient in its support for semantic interoperability.
\end{enumerate}
\textbf{Implications:}
\begin{enumerate}
  \item The specification of the data semantics is only dependent on the agent’s own perspective on the application domain, and can therefore be fulfilled before any interoperability with communication peers;
  \item No matter the number of different communication peers, the software agent needs to specify the semantics of its data elements only once;
  \item ... .
\end{enumerate}  
\end{mmdp}


We argued in [@Brandt2018a] that since software is incapable of genuine understanding, semantics cannot exist in software. Nevertheless, the software agent acts as transport medium for the semantics: for single-user software as medium to transport the semantics as it was intended by the software engineer to the semantics as it is experienced by the end user at the human-machine interface; for multi-user software as medium to transport the semantics as intended by one end user at the time of data insertion, to the semantics as experienced by another end user when retrieving the processed data. To act as valid transport medium for semantics, we further stated that the reciprocity between code and data does manifest itself as software semantics. We explained that by observing that data and code are always tightly coupled and since their reciprocity emerges as software behaviour, software malfunction originates (amongst others) from a broken reciprocity, i.e., inconsistencies between data and code. Consequently, when the data and code are representations of the things and rules in the application domain, their reciprocity represents the degree with which the collective outcome of processing all potential data refers to the intended states of affairs in reality. Any incoherent reciprocity equates to unfaithfulness: semantics that are considered invalid in the application domain. 

Despite the quality with which the data and the code are developed individually, we can maximise semantic validity by maximising their reciprocity, viz. demanding maximal coherence between code and data. We have called this the *semantic coherence principle*. The consequence of demanding high coherence between the data and its processing code is in its inevitably emerging monolith, which we denoted as the Atomic Semantic Monolith (ASM): a semantic monolith, for it refers to the monolith’s reciprocity between data and its processing code that describe the affairs in the application domain; Atomicity refers to the level of granularity at which the entity that is referred to by the data token is considered a non-dividable whole in the application domain. Where it is the objective of sIOP to address this monolithic nature of the ASM, as we do in the next section, it is the objective of semantics to maximise and maintain the coherence of the ASM, as elaborated in [ibid.].

Regarding the quality of the data and code models we reasoned that the data model should have a backward-looking role (in contrast to forward-looking) [@Gonzalez-Perez2007], present an ontological mode of modelling as opposed to a linguistic mode [@Atkinson2003], and demand a strong type-mode (as opposed to a token-mode) that result in non-transitivity and use the kind of abstraction known as classification [@Henderson-Sellers2012]. From those demands, we concluded that for representing semantics ontologies are best suited [@Brandt2018a]. For example, the trueness of forward-looking models, i.e., all 42010:2011 models, is established against their meta-models, while the trueness of backward-looking models, i.e., ontologies, is established through the interpretation in the conceptualisation of reality. 


----
Older text (kill your darlings)

    1. Models are expressed in terms of their modelling language which is formulated as meta-model. A meta-model specifies the different language constructs that are allowed and together the constructs specify the differences that can be expressed by the language. These differences are stable for the subject of the model, and for semantic models these therefore represent distinctions that are considered stable over the domain of application, e.g., between *entities* and their *relations*. This implies that the meta-model, as modelling language, commits to the existence of certain types of things that exist in reality, no more and no less. This characteristic of any language is known as its *ontological commitment*. Any other distinctions must be added to the model by text annotations, the interpretation of which remains outside the software’s capability.
    1. Maintaining consistency between data and code, and acknowledging the ontological commitment of the modelling language, implies that the code should first and foremost respect the primary differences that are carved out by the ontological commitment. The better the code is capable to maintain, even enforce, the differences that are expressed by the modelling language, the higher the coherence with the data. 
        1. ontological neutral languages such as Description Logic (or its computational representation as OWL), do not maintain an ontological commitment. Different families of logic is all there is to it, e.g., SHOINC versus ALC. This is about logical strength and its limits on what conclusions to draw and the consequential computational complexity required for that.
        1. an ontological commitment can be explicitly designed, mostly represented as an ontology and then denoted as foundational ontology (or top-level ontology). Examples are UFO, BFO, OBO, DOLCE. These ontological commitment are strongly influenced by philosophy, and some of them account for different philosophical stances and are, therefore, not comparable with each other.
    1. Coding is done using a programming language and a compiler. Respecting the ontological commitment of the modelling language can be implemented in different ways, and the most seamless of all is the approach where the distinct elements from the ontological commitment are an integral part of the programming language whereas their expressed differences are encoded in the compiler. The software engineer is then relieved from the responsibility to implement the interpretation for the ontological commitment herself.
    1. The models that are applied in MDA/MDE, viz. 42010:2011 models, are so-called prescriptive models, the trueness of which is first and foremost established against their meta-models. Oppositely, the trueness of descriptive models, i.e., ontologies, is laid in reality; a reality that has been represented with mathematical rigour. This rigour does not make for faultless models, but it does allow for a very stringent interpretation and validation. It follows that ontologies are better suited in support of maintaining and controlling the consistency between data and code than prescriptive models.    
1. Modelling reality requires established through the mathematical rigour provided by formal semantics
1. The application of the MOF as meta-model foundation acknowledges the existence of *things* and their *properties*, with *relations* between things as a specialisation of a thing, and an *instantiation* of a thing to represent a particular instance of that thing. When applying the meta-model as modelling language to model reality, its modelling language commits to the existence of these four types of things, only. Any model can then only distinguish between these four atomic aspects, and requires textual annotation to make further distinctions. This implies that we cannot distinguish in our model between a cup and the coffee that it holds, while their intuitive differences are foundational to their existence and their very different behaviour and potential use. This so-called *ontological commitment* of the modelling language is directly related to its expressibility and, subsequently, the semantic accuracy with which one can model reality.       




We provide guidance to their design and integration in contemporary architectural paradigms, with a position of the human-in-the-loop that is reduced and once-only.</Text>
    </Snapshot>
    <Snapshot Date="2020-06-29 12:01:16 +0200">
        <Title>More darlings killed</Title>
        <Text>It is clear that sIOP encompasses a communication between at least two actors. This brings a natural responsibility for both actors in the communication, described by [@Grice:1991BT] as the particular purpose of communication, viz. to serve:

1. Quantity: Make your contributions as informative as is required (for the current purpose of the exchange), and not more than is required;
2. Quality: Do not say what you believe to be false, or for which you lack evidence;
3. Relation: Be relevant (to the immediate needs);
4. Manner: Avoid obscurity of expression, ambiguity, and be brief and orderly.

The maxim’s Quantity and Manner immediately lead to the definition of a design principle to their effect, and applying the normative notation from [@Greefhorst2011] this reads as follows:

\begin{mmdp}[The responsibility for the semantic meaning of data lays with the source]\label{dp:rfsm}

When it is reasonable to expect that the software agent will be engaged in collaboration or otherwise will interoperate with (an)other software agent(s), it is the responsibility of the software architect to serve the quantity and manner of the potential interoperability by specifying the semantics of the data in advance. 

\textbf{Type of information:} business, data  \\
\textbf{Quality attributes:} semantics, interoperability, usability, efficiency   \\
\textbf{Rationale:}
\begin{enumerate}
  \item Data represent the state of affairs of some part of the world, viewed from a particular perspective of use. Such view is just one particular perspective out of many equally legitimate ones;
  \item Semantic heterogeneity, a direct consequence of the equally legitimate perspectives on reality, should not be considered a bug to resolve, but a feature to preserve and nurture in order to maximise semantic clarity and accuracy;
  \item Accepting semantic heterogeneity implies the probable uniqueness of the agents view on reality;
  \item Computers are not capable of genuine understanding, hence cannot establish semantics from data and thus require the human-in-the-loop for that;
  \item The responsibility for formulating the semantics that are expressed by the data can only lay by the software architect that has taken the particular perspective on reality when carving out the entities of interest to the software application;
  \item On specifying semantics, Grice’s maxims on communication, and particularly on serving the quantity and manner of communication, represent the natural constraints to respect;
  \item Without adherence to this principle, the meaning of the data expressed by the software agent can be considered flawed, inaccurate, incomplete or otherwise insufficient in its support for semantic interoperability.
\end{enumerate}
\textbf{Implications:}
\begin{enumerate}
  \item The specification of the data semantics is only dependent on the agent’s own perspective on the application domain, and can therefore be fulfilled before any specific interoperability with communication peers;
  \item No matter the number of different communication peers, the software agent needs to have the semantics of its data elements specified only once;
  \item By providing an explicit semantic specification of the data, an agent facilitates other components and agents to connect to it and, consequently, grounds its semantic interoperability with them unequivocally.
\end{enumerate}  
\end{mmdp}

We argued in [@Brandt2018a] that since software is incapable of genuine understanding, semantics cannot exist in software. Nevertheless, the software agent acts as transport medium for semantics between users. To act as valid transport medium for semantics, we further stated that the reciprocity between code and data does manifest itself as software semantics. This essential disposition discerns in semantics its *semantic meaning*, i.e., what is said and carried by data, and the *pragmatic meaning*, i.e., to connect with our frame of reference, the context of use, carried by code that implements comprehension as an inference process from the semantic meaning [@Grice:1991BT]. We explained that since data and code are always tightly coupled and since their reciprocity emerges as software behaviour, software malfunction originates (amongst others) from a broken reciprocity, i.e., inconsistencies between data and code. Consequently, when the data and code are representations of the things and laws in the application domain and, hence, represents semantic meaning and pragmatic meaning, their reciprocity represents the degree with which the collective outcome of processing all potential data refers to the intended states of affairs in reality. Any incoherent reciprocity equates to unfaithfulness: semantics that are considered invalid in the application domain. 

Despite the quality with which the data and the code are developed individually, we can maximise semantic validity by maximising their reciprocity, viz. demanding maximal coherence between code and data. We have called this the *semantic coherence principle*. The consequence of this principle is the emergence of what we denoted the *Atomic Semantic Monolith* (ASM): being a semantic monolith refers to the tight coupling of the reciprocity between data and their processing code which, together, describe the state of affairs in the application domain; Atomicity refers to the level of granularity at which the entity that is referred to by the data token is considered a non-dividable whole in the application domain. Where it is the objective of semantics to maximise and maintain the coherence of the ASM, as elaborated in (ibid.), it is the objective of sIOP to address this monolithic nature of the ASM, as we do in the next section.

-------------------------
More darlings killed:
* semantics are modelled by ontologies
* ontological commitment en OC design principle
-------------------------

~~~
Regarding the quality of the data and code models we reasoned that the data model should have a backward-looking role (in contrast to forward-looking) [@Gonzalez-Perez2007], present an ontological mode of modelling as opposed to a linguistic mode [@Atkinson2003], and demand a strong type-mode (as opposed to a token-mode) that result in non-transitivity and use the kind of abstraction known as classification [@Henderson-Sellers2012]. From those demands, we concluded that for representing semantics ontologies are best suited [@Brandt2018a]. For example, the trueness of forward-looking models, i.e., all 42010:2011 models, is established against their meta-models, while the trueness of backward-looking models, i.e., ontologies, is established through the interpretation in the conceptualisation of reality. 

Moreover, we showed (ibid.) that the predominant purpose of a model is to describe reality by distinguishing the entities of interest. These distinctions can be modelled, but the (modelling) language itself is also used to convey distinctions. The distinctions that are already articulated by the elementary language constructs define the expressiveness of that language; the more distinctions the language elements can convey, the more differences can be represented by that language. The (fundamental) categories that the language elements can discern apply during modelling as a commitment, e.g., the language that explicitly differentiate substantials into objects and amounts of matter commits to the intuitive difference between the cup and the coffee that it holds. When the modelling language does not commit to such distinctions, the user of the language is forced to specify these distinctions in the model itself. This so-called *ontological commitment* of the language [@Bricker2016; @Guarino1994,Guarino:1998wq] lays the foundations for the model and its data and, consequently, for the code to process the data. Interoperating peer agents that apply different ontological commitments will therefore show major differences in the construction and internals of their respective ASM’s. This observation leads to the following design principle:

\begin{mmdp}[Maintain an explicit ontological commitment]\label{dp:meoc}

The language constructs that are used to formulate a model always represent an ontological commitment, explicitly or implicitly.

\textbf{Type of information:} business, application, data  \\
\textbf{Quality attributes:} semantics, semantic interoperability, reliability   \\
\textbf{Rationale:}
\begin{enumerate}
  \item The particular reciprocity that emerges in the ASM is influenced by the ontological commitment of the modelling language;
  \item The purpose of sIOP is to re-establish a coherent reciprocity between the external semantic meaning and the internal pragmatic meaning;
  \item Incompatibility between the ontological commitments of both interoperating agents creates a sIOP concern on the modelling language level;
  \item sIOP cannot be established without having addressed this language concern;
  \item This language concern and its related resolution is independent from any particular sIOP case;
  \item By maintaining an explicit ontological commitment, its incompatibility with other ontological commitments can be addressed in a generic manner.
\end{enumerate}
\textbf{Implications:}
\begin{enumerate}
  \item Since the choice for a specific ontological commitment is only dependent on its applicability to the agent’s semantics, its specification therefore is independent from any specific interoperability case;
  \item No matter the number of different communication peers, the software agent needs to specify its ontological commitment only once;
  \item By specifying its ontological commitment explicitly, an agent enables the emergence of a standard and related infrastructural components to address this concern and to provide for reconciliation of differences between ontological commitments.
\end{enumerate}  
\end{mmdp}

~~~

Based on the principles and arguments in this section, we defend that software agents that might engage in interoperability should provide for a semantic anchorage in the form of a domain ontology and a foundational ontology; the latter to explicate the ontological commitment and the former to specify the semantics of the data. Such ontology provides the ability to connect to the semantics of the agent in a computational manner, consolidating the semantic concerns for semantic interoperability.
</Text>
        <Comments>brandtp, 22-6-2020 1: Summarise our previous paper; 2:present the underlying issues (separate syntax from semantics, i.e, avoidn tight coupling between semantics of comm.peers; ...); 3. application of mediation pattern as solution. 
brandtp, 22-6-2020 We did this already in the previous paper. Remove DP here, recall Language appropriateness principle.</Comments>
    </Snapshot>
</SnapshotIndexes>