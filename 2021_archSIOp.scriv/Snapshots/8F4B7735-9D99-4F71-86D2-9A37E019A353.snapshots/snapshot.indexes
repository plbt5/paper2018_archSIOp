<?xml version="1.0" encoding="UTF-8"?>
<SnapshotIndexes Version="1.0" BinderUUID="8F4B7735-9D99-4F71-86D2-9A37E019A353">
    <Snapshot Date="2018-05-23 15:29:05 +0200">
        <Title>Pre-Sync External File Version</Title>
        <Text>Still, the most popular solution towards SIOp is to establish a convention on the semantics of the terms that are used during the collaboration. This convention "resolves" the grounding problem in that it represents the know-how to "decode" the data, i.e., to connect the term with what it stands for in reality. Be aware that the realm of that know-how, denoted here as *background knowledge*, represents a semantic monolith, and it is being demarcated by its use in design time only: any software engineer who is made aware of that background knowledge, through bilateral agreements, domain-specific standards or other conventions, can design and implement measures to apply the data in accordance to what they refer to in reality. This solution is not wrong *per se*, and has as bonus that it does away with the consequences of the unresolved grounding problem because each and every term has already been grounded to everybody's satisfaction. In fact, for long-standing and stable collaborations between software agents, which live in business domains that experience only marginal changes in their business, semantic conventions are the most appropriate approach towards SIOp. Nevertheless, when foreign data emerge that are to be used within the semantic monolith, or when the native data are to be used outside the semantic monolith, SIOp, and hence the IT, will fail. SIOp fails because the convention was not shared to these foreign software agents. Moreover, any run-time attempt to communicate the background knowledge that is reflected by the convention is doomed to fail, because knowledge is just data and thus represented by terms that, in this case, are ungrounded. Again, human intervention is required. This time its purpose is not to resolve the grounding problem, but to bridge the semantic gap that originates from differences in groundings between each pair of collaborating software agents. This is called *semantic reconciliation*, and results in an *alignment* between both groundings. Although weak AI can support in that task [@Euzenat:2013ie], due to its inherent limitations it cannot fulfill the task completely without human intervention. In conclusion, to break the semantic monolith and provide a pair of software agents with SIOp, human intervention is required to produce an alignment between the semantic groundings of each software agent. The issues in this matter relate to (i) the inherent conflict between having a human in the loop and an access-and-play demand; (ii) ***analyse the paragraph below and formulate its issues concretely here*** .
</Text>
    </Snapshot>
    <Snapshot Date="2018-05-25 15:33:16 +0200">
        <Title>Pre-Sync External File Version</Title>
        <Text>Still, the most popular solution towards SIOp is to establish a convention on the semantics of the terms that are used during the collaboration. This convention "resolves" the grounding problem in that it represents the know-how to "decode" the data, i.e., to connect the term with what it stands for in reality. Be aware that the realm of that know-how, denoted here as *background knowledge*, represents a semantic monolith, and it is being demarcated by its use in design time only: any software engineer who is made aware of that background knowledge, through bilateral agreements, domain-specific standards or other conventions, can design and implement measures to apply the data in accordance to what they refer to in reality. This solution is not wrong *per se*, and has as bonus that it does away with the consequences of the unresolved grounding problem because each and every term has already been grounded to everybody's satisfaction. In fact, for long-standing and stable collaborations between software agents, which live in business domains that experience only marginal changes in their business, semantic conventions are the most appropriate approach towards SIOp. Nevertheless, when foreign data emerge that are to be used within the semantic monolith, or when the native data are to be used outside the semantic monolith, SIOp, and hence the IT, will fail. SIOp fails because the convention was not shared to these foreign software agents. Moreover, any run-time attempt to communicate the background knowledge that is reflected by the convention is doomed to fail, because knowledge is just data and thus represented by terms that, in this case, are ungrounded. Again, human intervention is required. This time its purpose is not to resolve the grounding problem, but to bridge the semantic gap that originates from differences in groundings between each pair of collaborating software agents. This is called *semantic reconciliation*, and results in an *alignment* between both groundings. Although weak AI can support in that task [@Euzenat:2013ie], due to its inherent limitations it cannot fulfill the task completely without human intervention. In conclusion, to break the semantic monolith and provide a pair of software agents with SIOp, human intervention is required to produce an alignment between the semantic groundings of each software agent. The issues in this matter relate to (i) the inherent conflict between having a human in the loop and an access-and-play demand; (ii) ***analyse the paragraph below and formulate its issues concretely here*** .

</Text>
    </Snapshot>
    <Snapshot Date="2018-06-11 13:15:09 +0200">
        <Title>Pre-External File Sync Overwrite</Title>
        <Text>[1] M. B. Almeida, C. P. Pessanha, and R. Barcelos, “Information Architecture for Organizations: An Ontological Approach,” in Ontology in Information Science, C. Thomas, Ed. IntechOpen, 2018, pp. 1–27.

[2] S. Yang, J. Guo, and R. Wei, “Semantic interoperability with heterogeneous information systems on the internet through automatic tabular document exchange,” Inf. Syst., vol. 69, pp. 195–217, Sep. 2017.

[3] U. Aßmann, S. Zschaler, and G. Wagner, “Ontologies, Meta-models, and the Model-Driven Paradigm,” in Ontologies for Software Engineering and Software Technology, C. Calero, F. Ruiz, and M. Piattini, Eds. Springer-Verlag Berlin Heidelberg, 2006, pp. 249–273.

[4] C. Atkinson and T. Kühne, “The Essence of Multilevel Metamodeling,” LNCS, vol. 2185, pp. 19–33, 2001.</Text>
    </Snapshot>
    <Snapshot Date="2022-10-03 10:27:49 +0200">
        <Title>Pre restructure to align with Concerns</Title>
        <Text>Discuss the following papers:

1. https://scholar.google.com/citations?user=pkJy5p8AAAAJ&amp;hl=en
1. Pahl, C. (2007). Semantic model-driven architecting of service-based software systems. Information and Software Technology, 49(8), 838–850. https://doi.org/10.1016/J.INFSOF.2006.09.007 —&gt; Compare how they apply onto’s for DM

1. Maybe discuss as agreement-based approach to sIOP: J.A. Mykkänen, M.P. Tuomainen, An evaluation and selection framework for interoperability standards, Information and Software Technology, Volume 50, Issue 3, 2008,Pages 176-197, ISSN 0950-5849, https://doi.org/10.1016/j.infsof.2006.12.001. (https://www.sciencedirect.com/science/article/pii/S0950584906001960)

Several works exist that focus on logical foundations of sIOP in search for automatic matching and merging of ontologies [@Euzenat:2013ie;@Benedikt2018a;@Scharffe2014], or for automatic detection and reconciliation of semantic inconsistencies between ontologies [@Fahad2012;@Diggelen:2007vd]. These works lay at the heart of ontology mediation, however, their embedding in the larger software environment is not addressed or only assumed at best. At the same time, several works exist that address the architectural foundations related to information analyses and interoperability, both in academic literature [@Karagiannis2006;@Raghupathi2008] and in industrial practices [@ObjectManagementGroupOMG2013]. Few literature exists on on the combination of foundations for architectural concerns and those for formal semantics, and these make use of ontological foundations to specify the semantics of the capabilities and resources of the architectural language as opposed to domain semantics [@Naudet2010;@Azevedo2015;@Carvalho2016]. 

The authors of [@Pagano2013a] present an abstract account of interoperability issues, research challenges and fundamental factors that apply in interoperability. In terms of these findings, our work can be considered on-topic, relevant, and in alignment with their research agenda. It distinguishes from theirs by addressing sIOP solutions to part of their broader and more abstract view on interoperability. 

In [@Horsch2020] the authors discuss the ongoing work on establishing a European Virtual Marketplace Framework, into which diverse platforms can be integrated. It addresses common challenges that arise when marketplace-level domain ontologies are combined with a top-level ontology like the European Materials and Modelling Ontology (EMMO) by ontology alignment. A multi-tier system of ontologies is established with the EMMO at the top and all others subsumed by it. The authors show that with such a setup the top-level ontology is crucial in the creation of the alignments between the (domain)ontologies that are subsumed by it. At the one hand this shows support to our conclusion that semantic compatibility can be considered foundational to semantic interoperability. At the other hand their particular approach is a centralised one that does not scale well in large distributed environments due to its dependency on one single ontological commitment. Moreover, the top-level ontology, here EMMO, necessarily conflates its function as ontological commitment with a function to construct alignments from. Although this is of great help to resolving the (automated) ontology matching problem, it creates a semantic monolith that extends to all communicating peers which, as we have seen in \cref{introduction}, impedes not only access-and-play sIOP but semantic scalability, evolvability, maintainability and other qualities as well. 

The automatic tabular document exchange (DocEx) framework proposed by \cite{Yang2017} divides semantic interoperability into two stages: *interpretation*, described as automatic unambiguous information understanding, and *employment*, understood as the capability to automatically operate on the information according to the interpreted semantics. The interpretation phase is dependent on a global vocabulary that “provides uniquely coded and unambiguous concepts across different domains”. Essentially, this is a clear example of the *semantic standard fallacy* described by \cite{Janowicz:2013ui}: “The successful standardisation of protocols made us believe that we should also standardise meaning on the Web. This is a fundamental misconception”, particularly since it defies semantic heterogeneity and different but equally legitimate perspectives on the same thing. The authors remind us of three limitations of the ontology alignment approach; firstly, it cannot guarantee complete semantic interoperability for situations where terms are not aligned; secondly, creating alignments are time consuming; thirdly, ontologies are often local and their point-to-point alignments limits the semantic consistency on a more global scheme. While we do not deny any of these we consider that (i) alignments exist to facilitate interoperability, hence, lacuna are to be (automatically) corrected; (ii) their creation, despite ontology matching algorithms and other automation, will take time but allow for local semantic qualities and independence that are impossible to achieve with a global semantic standard; (iii) local applications are not seeking for global interoperability but business network interoperability only. 

The DocEx framework can be considered a simplified version of the openEHR framework^[https://www.openehr.org/, accessed Jan 24, 2020] as introduced by [@Beale:2001vz], further elaborated in [@Beale2007a;@Beale2008a;@Beale2007;@Beale2008;@Beale2007b;@Beale2007c] and incorporated into CEN 13606 as a European and ISO standard. Its founding key paradigm is to model generic knowledge apart from the specific information structures, and let the former constrain the latter: knowledge is expressed as “statements which say how instances of a reference model should be constrained to form a valid business entity of some kind”. Those statements are embodied by *archetypes*. They introduce a Reference Model (RM) that can be semantically constrained by an Archetype Model (AM). The latter can be considered a meta-model or modelling language to express archetypes, i.e., a particular semantic model representing knowledge. The former is provided to each stakeholder as a unified software implementation (”the run-time platform”), providing invariant patterns of information structures. This separation makes what the RM is to the AM similar to what the JVM is to the java program. Any information item created by a user is registered as an instance of RM-specified invariant patterns. At the same time that information item is conforming to an archetype that expresses (constrains) its semantics in terms of the AM. Such approach follow \cref{dp:rfsm,dp:mediation} but the application of a central definition of archetypes maintain a tight coupling and thus defies semantic heterogeneity and truly independent semantic representations. 

The authors in [@Haller2005] propose the Web Service Execution Environment (WSMX) that enables the execution of Semantic Web Services based on a Web Service Modelling Ontology (WSMO), and consider it a reference implementation for WSMO. It is meant as a means for automated discovery, composition and execution of Web Services which are based on logical inference-mechanisms, and in this way similar to our objective. Another similarity is in their acceptance of semantic heterogeneity and the need for a generic data mediator to overcome semantic differences, thereby following \cref{dp:ssoc}. Despite these similarities, we consider two main differences with our approach. Firstly, the goals of WSMO are of another, broader, dimension than our goal to consolidate semantic interoperability and for which we have introduced details that are out of scope of WSMO. Secondly,

Recently the international data spaces (IDS) association^[https://www.internationaldataspaces.org/the-principles/#overview, accessed Jan 28, 2020] forms the basis for a data marketplace as a strategic link between the creation of data in the internet of things and applying this data in machine learning (ML) and artificial intelligence (AI) algorithms. The proposed architecture, [@Otto2019], is much alike the WSMX in the sense that a well-defined connector provides infrastructural services concerning security and trust, sovereignty, interoperability, ease of adoption and use, and more. In fact, if we conceive WSMX as an abstracted version of Web Services with a particular attention to semantics, IDS can be conceived as an abstracted version of the REST framework that considers data resources (spaces) the central assets in an ecosystem. IDS puts a particular attention to technology transparency when it comes to asset discovery and disclosure, identity, their secure, managed and accountable use, and interoperability. IDS considers data as assets, and provides many if not all necessary components for its managed exchange. Contrarily, we observe a clear absence of any considerations similar to those we bring forward in this paper towards the consolidation of sIOP. Still, and like WSMX’s and our objectives, IDS clearly intends to put forward an architectural design with the aim to solve the concerns generically into a transparent infrastructure. In conclusions, we consider IDS’ data technology transparency an interesting complement to our data semantic transparency.

[@Greefhorst2011] provides for a Principles Catalogue in their appendix which presents almost sixty design principles. Principle A.20 of them reads *Data that are exchanged adhere to a canonical data model*, and one of its rationals states that *A Canonical Data Model standardises the definitions of data that are exchanged (...)*. Indeed, principle A.20 reflects the current practises to achieve interoperability. Although it conflicts with \cref{dp:ssoc}, it is not necessarily wrong, since following it results in achieving interoperability as justified by the many if not all interoperable systems that exist today. However, its application impedes access-and-play interoperability as well as semantic heterogeneity, as we have shown in \cref{waar-precies}.

In support of EU’s single digital market, the new European Interoperability Framework (EIF) [@EU-ISA2Program2019] gives guidance to public administrations on how to improve interoperability. It provides three recommendations on the semantic level which are rather abstract in their formulation, although exemplifying these with approaches that can be categorised in FEI [@Chen2017] as integrated, or unified at best, but not federated. These examples imply that our approach is not in compliance with the EIF, which is in contrast with the three recommendations that do not exclude a federated approach, or any other of the DPs that we have elaborated.

The authors of [@Neiva2016] present a literature research to pragmatic interoperability. Although no definition on pragmatics or its interoperability is given, the distinction with semantics is given as “semantics is related with which the [semiotic] sign refers to, and pragmatics is related to the effect of the sign on the interpreter.”. This aligns with our application of semantic meaning and pragmatic meaning. Where the authors keep pragmatics separate from semantics and, therefore, treat pragmatic interoperability separate from semantic interoperability, we consider the reciprocity between them significant and conclude that semantic interoperability can only be achieved when the DSC can re-establish the connection of the DSP’s data with its own pragmatic meaning. Opposed to the authors opinion, we consider the pragmatic meaning a necessary element in achieving sIOP. 

A quasi-systematic literature review to interoperability in context-aware software systems was undertaken by [@Motta2019], resulting in an Interoperability Theoretical Framework “used to guide the evolution of software systems regarding changes focused on interoperability”. They identify 16 high-level interoperability mechanisms, i.e., decisions to be taken and solutions recurrently used throughout the software systems to achieve interoperability. Our design principles are in (partial) support of 12(3) of the mechanisms, i.e., “Use common ontologies to enable understanding”, and “Use ‘bridges’ to enable interaction”. The 4 mechanisms that we do not support can be considered out of our scope, e.g., “Use opens source solutions” or “Use suitable protocols”. </Text>
    </Snapshot>
    <Snapshot Date="2018-05-23 15:24:27 +0200">
        <Title>Pre-External File Sync Overwrite</Title>
        <Text>Still, the most popular solution towards SIOp is to establish a convention on the semantics of the terms that are used during the collaboration. This convention "resolves" the grounding problem in that it represents the know-how to "decode" the data, i.e., to connect the term with what it stands for in reality. Be aware that the realm of that know-how, denoted here as *background knowledge*, represents a semantic monolith, and it is being demarcated by its use in design time only: any software engineer who is made aware of that background knowledge, through bilateral agreements, domain-specific standards or other conventions, can design and implement measures to apply the data in accordance to what they refer to in reality. This solution is not wrong *per se*, and has as bonus that it does away with the consequences of the unresolved grounding problem because each and every term has already been grounded to everybody's satisfaction. In fact, for long-standing and stable collaborations between software agents, which live in business domains that experience only marginal changes in their business, semantic conventions are the most appropriate approach towards SIOp. Nevertheless, when foreign data emerge that are to be used within the semantic monolith, or when the native data are to be used outside the semantic monolith, SIOp, and hence the IT, will fail. SIOp fails because the convention was not shared to these foreign software agents. Moreover, any run-time attempt to communicate the background knowledge that is reflected by the convention is doomed to fail, because knowledge is just data and thus represented by terms that, in this case, are ungrounded. Again, human intervention is required. This time its purpose is not to resolve the grounding problem, but to bridge the semantic gap that originates from differences in groundings between each pair of collaborating software agents. This is called *semantic reconciliation*, and results in an *alignment* between both groundings. Although weak AI can support in that task [@Euzenat:2013ie], due to its inherent limitations it cannot fulfill the task completely without human intervention. In conclusion, to break the semantic monolith and provide a pair of software agents with SIOp, human intervention is required to produce an alignment between the semantic groundings of each software agent. The issues in this matter relate to (i) the inherent conflict between having a human in the loop and an access-and-play demand; (ii) ***analyse the paragraph below and formulate its issues concretely here*** . 
</Text>
    </Snapshot>
    <Snapshot Date="2018-05-23 23:35:38 +0200">
        <Title>Pre-Sync External File Version</Title>
        <Text>Still, the most popular solution towards SIOp is to establish a convention on the semantics of the terms that are used during the collaboration. This convention "resolves" the grounding problem in that it represents the know-how to "decode" the data, i.e., to connect the term with what it stands for in reality. Be aware that the realm of that know-how, denoted here as *background knowledge*, represents a semantic monolith, and it is being demarcated by its use in design time only: any software engineer who is made aware of that background knowledge, through bilateral agreements, domain-specific standards or other conventions, can design and implement measures to apply the data in accordance to what they refer to in reality. This solution is not wrong *per se*, and has as bonus that it does away with the consequences of the unresolved grounding problem because each and every term has already been grounded to everybody's satisfaction. In fact, for long-standing and stable collaborations between software agents, which live in business domains that experience only marginal changes in their business, semantic conventions are the most appropriate approach towards SIOp. Nevertheless, when foreign data emerge that are to be used within the semantic monolith, or when the native data are to be used outside the semantic monolith, SIOp, and hence the IT, will fail. SIOp fails because the convention was not shared to these foreign software agents. Moreover, any run-time attempt to communicate the background knowledge that is reflected by the convention is doomed to fail, because knowledge is just data and thus represented by terms that, in this case, are ungrounded. Again, human intervention is required. This time its purpose is not to resolve the grounding problem, but to bridge the semantic gap that originates from differences in groundings between each pair of collaborating software agents. This is called *semantic reconciliation*, and results in an *alignment* between both groundings. Although weak AI can support in that task [@Euzenat:2013ie], due to its inherent limitations it cannot fulfill the task completely without human intervention. In conclusion, to break the semantic monolith and provide a pair of software agents with SIOp, human intervention is required to produce an alignment between the semantic groundings of each software agent. The issues in this matter relate to (i) the inherent conflict between having a human in the loop and an access-and-play demand; (ii) ***analyse the paragraph below and formulate its issues concretely here*** .
</Text>
    </Snapshot>
    <Snapshot Date="2018-05-23 23:29:00 +0200">
        <Title>Pre-External File Sync Overwrite</Title>
        <Text>Still, the most popular solution towards SIOp is to establish a convention on the semantics of the terms that are used during the collaboration. This convention "resolves" the grounding problem in that it represents the know-how to "decode" the data, i.e., to connect the term with what it stands for in reality. Be aware that the realm of that know-how, denoted here as *background knowledge*, represents a semantic monolith, and it is being demarcated by its use in design time only: any software engineer who is made aware of that background knowledge, through bilateral agreements, domain-specific standards or other conventions, can design and implement measures to apply the data in accordance to what they refer to in reality. This solution is not wrong *per se*, and has as bonus that it does away with the consequences of the unresolved grounding problem because each and every term has already been grounded to everybody's satisfaction. In fact, for long-standing and stable collaborations between software agents, which live in business domains that experience only marginal changes in their business, semantic conventions are the most appropriate approach towards SIOp. Nevertheless, when foreign data emerge that are to be used within the semantic monolith, or when the native data are to be used outside the semantic monolith, SIOp, and hence the IT, will fail. SIOp fails because the convention was not shared to these foreign software agents. Moreover, any run-time attempt to communicate the background knowledge that is reflected by the convention is doomed to fail, because knowledge is just data and thus represented by terms that, in this case, are ungrounded. Again, human intervention is required. This time its purpose is not to resolve the grounding problem, but to bridge the semantic gap that originates from differences in groundings between each pair of collaborating software agents. This is called *semantic reconciliation*, and results in an *alignment* between both groundings. Although weak AI can support in that task [@Euzenat:2013ie], due to its inherent limitations it cannot fulfill the task completely without human intervention. In conclusion, to break the semantic monolith and provide a pair of software agents with SIOp, human intervention is required to produce an alignment between the semantic groundings of each software agent. The issues in this matter relate to (i) the inherent conflict between having a human in the loop and an access-and-play demand; (ii) ***analyse the paragraph below and formulate its issues concretely here*** .
</Text>
    </Snapshot>
</SnapshotIndexes>